# -*- coding: utf-8 -*-
"""Copy of Trial1 - Project NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1riZuBWpaobLut4C4XPL-pW-HyzzkQfRs
"""

# ============================================================================
# INDIC LANGUAGE UNLEARNING - GOOGLE COLAB NOTEBOOK
# ============================================================================
# Milestone 1: Lightweight model for testing
# Languages: 2 High-resource (Hindi, Bengali) + 2 Low-resource (Maithili, Bhojpuri)
# ============================================================================


# # üöÄ Indic Language Unlearning - Milestone 1
#
# This notebook tests selective forgetting in multilingual models using:
# - **High-resource**: Hindi (hi), Bengali (bn)
# - **Low-resource**: Maithili (mai), Bhojpuri (bho)
#
# Using a **lightweight model** (BLOOM-560m) for faster experimentation.

!pip install -q torch lightning transformers datasets pandas torchmetrics sentencepiece accelerate

from google.colab import drive
drive.mount('/content/drive')

# Create project directories
!mkdir -p /content/drive/MyDrive/indic_unlearning/checkpoints_fixed
!mkdir -p /content/drive/MyDrive/indic_unlearning/data
!mkdir -p /content/drive/MyDrive/indic_unlearning/cache

print("‚úì Directories created")

import os

FLORES_PATH = "/content/drive/MyDrive/indic_unlearning/flores200_dataset"

if os.path.exists(FLORES_PATH):
    print("‚úì FLORES-200 dataset found!")
    print("\nChecking for required language files:")
    !ls {FLORES_PATH}/devtest/ | grep -E "hin|ben|tel|mai|bho"
else:
    print("‚úó FLORES-200 dataset NOT found!")
    print(f"\nPlease upload your flores200_dataset folder to:")
    print(f"  Google Drive > indic_unlearning > flores200_dataset")

import json
import random
from pathlib import Path

FLORES_PATH = Path("/content/drive/MyDrive/indic_unlearning/flores200_dataset")
OUTPUT_DIR = Path("/content/drive/MyDrive/indic_unlearning/data")
SEED = 42

SELECTED_LANGUAGES = {
    "hi": "hin_Deva",
    "bn": "ben_Beng",
    "te": "tel_Telu",
    "mai": "mai_Deva",
    "bho": "bho_Deva",
}

FORGET_NUM = 50
RETAIN_MULTIPLIER = 5

def load_flores_sentences(flores_path, split="devtest"):
    data = {}
    for lang_code, flores_code in SELECTED_LANGUAGES.items():
        file_path = flores_path / split / f"{flores_code}.{split}"
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                sentences = [line.strip() for line in f if line.strip()]
            data[lang_code] = sentences
            print(f"‚úì Loaded {lang_code}: {len(sentences)} sentences")
        else:
            print(f"‚úó File not found: {file_path}")
    return data

def create_parallel_dataset(data):
    if not data:
        return []
    min_len = min(len(sentences) for sentences in data.values())
    parallel_data = []
    for i in range(min_len):
        sample = {lang: sentences[i] for lang, sentences in data.items()}
        parallel_data.append(sample)
    return parallel_data

def split_and_save(parallel_data, output_dir, forget_num, retain_multiplier, seed):
    random.seed(seed)
    random.shuffle(parallel_data)

    total = len(parallel_data)
    train_size = int(0.7 * total)
    valid_size = int(0.15 * total)

    train_data = parallel_data[:train_size]
    valid_data = parallel_data[train_size:train_size + valid_size]
    test_data = parallel_data[train_size + valid_size:]

    forget_data = train_data[:forget_num]
    retain_data = train_data[forget_num:forget_num + (forget_num * retain_multiplier)]

    output_dir.mkdir(parents=True, exist_ok=True)

    def save_jsonl(data, filepath):
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"  ‚úì Saved {len(data)} samples to {filepath.name}")

    save_jsonl(forget_data, output_dir / f"forget-{forget_num}.jsonl")
    save_jsonl(retain_data, output_dir / f"retain-{forget_num}-x{retain_multiplier}.jsonl")
    save_jsonl(valid_data, output_dir / "valid.jsonl")
    save_jsonl(test_data, output_dir / "test.jsonl")

    return len(forget_data), len(retain_data), len(valid_data), len(test_data)

print("="*60)
print("PREPARING INDIC FLORES DATA")
print("="*60)

data = load_flores_sentences(FLORES_PATH, "devtest")

if data:
    parallel_data = create_parallel_dataset(data)
    print(f"\n‚úì Created {len(parallel_data)} parallel samples")
    sizes = split_and_save(parallel_data, OUTPUT_DIR, FORGET_NUM, RETAIN_MULTIPLIER, SEED)
    print(f"\nDataset sizes: Forget={sizes[0]}, Retain={sizes[1]}, Valid={sizes[2]}, Test={sizes[3]}")
else:
    print("\n‚úó Data preparation failed.")

import os.path as osp
import random
import torch

# FIX: Remove explicit sympy installation to avoid version conflict with torch
# !pip install sympy==1.11.0

import lightning as L
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
from transformers import AutoTokenizer

FLORES_LANGUAGES = ["hi", "bn","te", "mai", "bho"]

class FLORESDataModule(L.LightningDataModule):
    SUPPORTED_LANGUAGES = FLORES_LANGUAGES

    def __init__(self, args):
        super().__init__()
        self.args = args
        self.tokenizer = AutoTokenizer.from_pretrained(
            args.model_name_or_path,
            cache_dir=args.cache_dir,
            use_fast=False,
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.flores_valid = []
        self.flores_test = []

    def setup(self, stage=None):
        if stage == "fit":
            forget_data = self._load_json(f"forget-{self.args.forget_num}.jsonl")
            retain_data = self._load_json(f"retain-{self.args.forget_num}-x{self.args.retain_multiplier}.jsonl")
            self.flores_forget = FLORESDataset(forget_data, self.tokenizer, self.args.max_seq_len, self.args.forget_lang)
            self.flores_retain = FLORESDataset(retain_data, self.tokenizer, self.args.max_seq_len, self.args.retain_lang)

            valid_data = self._load_json("valid.jsonl")
            for lang in self.args.forget_lang:
                self.flores_valid.append(FLORESDataset(valid_data, self.tokenizer, self.args.max_seq_len, lang))
                self.flores_valid.append(FLORESDataset(forget_data, self.tokenizer, self.args.max_seq_len, lang))

        if stage == "validate":
            forget_data = self._load_json(f"forget-{self.args.forget_num}.jsonl")
            valid_data = self._load_json("valid.jsonl")
            for lang in self.args.forget_lang:
                self.flores_valid.append(FLORESDataset(valid_data, self.tokenizer, self.args.max_seq_len, lang))
                self.flores_valid.append(FLORESDataset(forget_data, self.tokenizer, self.args.max_seq_len, lang))

        if stage == "test":
            forget_data = self._load_json(f"forget-{self.args.forget_num}.jsonl")
            test_data = self._load_json("test.jsonl")
            langs = self.args.forget_lang if self.args.test_src_lang_only else self.SUPPORTED_LANGUAGES
            for lang in langs:
                self.flores_test.append(FLORESDataset(test_data, self.tokenizer, self.args.max_seq_len, lang))
                self.flores_test.append(FLORESDataset(forget_data, self.tokenizer, self.args.max_seq_len, lang))

    def _load_json(self, filename):
        return load_dataset("json", data_files=osp.join(self.args.data_dir, filename), cache_dir=self.args.cache_dir)["train"]

    def train_dataloader(self):
        if self.args.alternate_loader_every_n_epoch:
            # FIXED: 1:1 ratio instead of 3:1
            if self.trainer.current_epoch % 2 == 1:
                dataset = self.flores_retain
            else:
                dataset = self.flores_forget
        else:
            dataset = self.flores_retain
        return DataLoader(dataset, batch_size=self.args.per_device_train_batch_size, num_workers=2, shuffle=True)

    def val_dataloader(self):
        return [DataLoader(ds, batch_size=self.args.per_device_eval_batch_size, num_workers=2) for ds in self.flores_valid]

    def test_dataloader(self):
        return [DataLoader(ds, batch_size=self.args.per_device_eval_batch_size, num_workers=2) for ds in self.flores_test]

class FLORESDataset(Dataset):
    def __init__(self, data, tokenizer, max_seq_len=128, lang="hi"):
        self.data = data
        self.tokenizer = tokenizer
        self.max_seq_len = max_seq_len
        self.lang = lang

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        lang = random.choice(self.lang) if isinstance(self.lang, list) and len(self.lang) > 1 else (self.lang[0] if isinstance(self.lang, list) else self.lang)
        text = self.data[idx][lang]

        inputs = self.tokenizer(text, max_length=self.max_seq_len, padding="max_length", truncation=True, return_tensors="pt")
        labels = inputs["input_ids"].clone()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {"input_ids": inputs["input_ids"].squeeze(), "attention_mask": inputs["attention_mask"].squeeze(), "labels": labels.squeeze()}

print("‚úì DataModule created")

import torch.nn.functional as F
from transformers import AutoModelForCausalLM, get_linear_schedule_with_warmup
from torchmetrics import Accuracy
import math

class ImprovedUnlearningModel(L.LightningModule):
    """
    FIXED: Prevents model collapse with:
    1. Adaptive loss scaling (starts gentle, increases gradually)
    2. Separate gradient clipping for forget/retain
    3. Regularization term to keep weights close to original
    4. Better monitoring of retain set performance
    """
    def __init__(self, args):
        super().__init__()
        self.save_hyperparameters(args)
        self.args = args

        print(f"Loading {args.model_name_or_path}...")
        self.model = AutoModelForCausalLM.from_pretrained(
            args.model_name_or_path,
            cache_dir=args.cache_dir,
        )

        # Enable gradient checkpointing if specified in args
        if args.use_gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
            print("‚úì Gradient checkpointing enabled.")

        # Save initial weights for regularization
        # FIXED: Register initial state buffers directly without .cpu() so they move with the model
        if args.use_weight_regularization:
            for name, p in self.model.named_parameters():
                # Register each parameter's initial state as a buffer.
                # Lightning will move these buffers to the correct device when the model is moved.
                self.register_buffer(f'initial_state_param_{name.replace(".", "_")}', p.data.clone().detach())

        self.tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir, use_fast=False)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Dataset names for logging
        self.valid_dataset_names = []
        self.test_dataset_names = []
        for lang in args.forget_lang:
            self.valid_dataset_names.extend([f"val/{lang}_", f"val/{lang}_forget_"])
        for lang in (args.forget_lang if args.test_src_lang_only else FLORES_LANGUAGES):
            self.test_dataset_names.extend([f"test/{lang}_", f"test/{lang}_forget_"])

        # Track best retain performance
        self.best_retain_ppl = float('inf')

    def get_adaptive_forget_weight(self):
        """
        CRITICAL FIX: Gradually increase forget loss weight
        Starts at 0.01, increases to max over warmup period
        """
        if not hasattr(self, 'trainer') or self.trainer is None:
            return self.args.forget_loss_weight

        current_step = self.trainer.global_step
        total_steps = self.trainer.estimated_stepping_batches
        warmup_steps = int(total_steps * 0.3)  # 30% warmup

        if current_step < warmup_steps:
            # Linear warmup from 0.01 to target weight
            progress = current_step / warmup_steps
            return 0.01 + (self.args.forget_loss_weight - 0.01) * progress
        else:
            return self.args.forget_loss_weight

    def forward(self, input_ids, attention_mask, labels):
        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)

    def training_step(self, batch, batch_idx):
        outputs = self(**batch)
        loss = outputs.loss

        # Check which dataset we're using (FIXED: 1:1 ratio)
        is_retain_epoch = self.current_epoch % 2 == 1

        if is_retain_epoch:
            # Normal training on retain set
            final_loss = loss
            self.log("train/retain_loss", loss, prog_bar=True)
            self.log("train/is_retain", 1.0)
        else:
            # Gradient ascent on forget set with ADAPTIVE scaling
            adaptive_weight = self.get_adaptive_forget_weight()
            forget_loss = -loss * adaptive_weight

            # REGULARIZATION: Add penalty for drifting too far from initial weights
            if self.args.use_weight_regularization:
                total_weight_diff = 0.0
                for name, p in self.model.named_parameters():
                    # FIXED: Directly access the registered buffer which is already on the correct device
                    initial_p_buffer = getattr(self, f'initial_state_param_{name.replace(".", "_")}')
                    # No need for .to(p.device) as the buffer should already be on the same device as p
                    total_weight_diff += F.mse_loss(p, initial_p_buffer)
                weight_diff = total_weight_diff
                reg_loss = self.args.weight_reg_strength * weight_diff
                final_loss = forget_loss + reg_loss
                self.log("train/weight_drift", weight_diff)
            else:
                final_loss = forget_loss

            self.log("train/forget_loss_raw", loss)
            self.log("train/forget_loss", final_loss, prog_bar=True)
            self.log("train/adaptive_weight", adaptive_weight, prog_bar=True)
            self.log("train/is_retain", 0.0)

        self.log("train/loss", final_loss)
        return final_loss

    def validation_step(self, batch, batch_idx, dataloader_idx=0):
        outputs = self(**batch)
        loss = outputs.loss
        logits = outputs.logits     # (B, seq_len, vocab_size)

        # clamp loss for stability (keep your existing behavior)
        loss = torch.clamp(loss, min=0.0, max=20.0)
        ppl = torch.exp(loss)
        ppl = torch.clamp(ppl, max=1e4)

        # --- CORRECT TOKEN-LEVEL ACCURACY FOR CAUSAL LM: SHIFTED COMPARISON ---
        # logits[:, t, :] is predicting token at position t+1, so compare:
        # predicted_tokens[:, :-1]  vs labels[:, 1:]
        predictions = torch.argmax(logits, dim=-1)   # (B, seq_len)
        labels = batch["labels"]                     # (B, seq_len) with -100 padding

        # shift predictions and labels
        pred_shift = predictions[:, :-1]
        label_shift = labels[:, 1:]

        # mask for valid (non-ignored) tokens after shift
        mask = label_shift != -100

        valid_tokens = mask.sum().float()
        if valid_tokens.item() > 0:
            correct = (pred_shift == label_shift) & mask
            ma = correct.sum().float() / valid_tokens * 100.0  # percentage
        else:
            ma = torch.tensor(0.0, device=loss.device)

        # Name for logger (preserve your indexing logic)
        name = self.valid_dataset_names[dataloader_idx] if dataloader_idx < len(self.valid_dataset_names) else f"val/{dataloader_idx}_"

        self.log(f"{name}loss", loss, add_dataloader_idx=False)
        self.log(f"{name}ppl", ppl, add_dataloader_idx=False)
        self.log(f"{name}ma", ma, add_dataloader_idx=False)

        # Optional: show a tiny decoded debug sample on the first validation batch only
        if batch_idx == 0 and (hasattr(self, "tokenizer") and self.tokenizer is not None):
            try:
                # pick first sample in batch
                sample_pred = pred_shift[0].cpu().numpy().tolist()
                sample_lab  = label_shift[0].cpu().numpy().tolist()
                # filter out -100 and decode a short prefix
                sample_lab_tokens = [t for t in sample_lab if t != -100][:20]
                sample_pred_tokens = sample_pred[:len(sample_lab_tokens)]
                print("DEBUG VAL SAMPLE:")
                print("  labels  >", self.tokenizer.decode(sample_lab_tokens, clean_up_tokenization_spaces=True))
                print("  predict >", self.tokenizer.decode([int(t) for t in sample_pred_tokens], clean_up_tokenization_spaces=True))
            except Exception:
                pass

        return {"loss": loss, "ppl": ppl, "ma": ma, "is_forget": "forget" in name}

    def test_step(self, batch, batch_idx, dataloader_idx=0):
        outputs = self(**batch)
        loss = outputs.loss
        logits = outputs.logits

        loss = torch.clamp(loss, min=0.0, max=20.0)
        ppl = torch.exp(loss)
        ppl = torch.clamp(ppl, max=1e4)

        # SHIFT for causal LM
        predictions = torch.argmax(logits, dim=-1)
        labels = batch["labels"]

        pred_shift = predictions[:, :-1]
        label_shift = labels[:, 1:]
        mask = label_shift != -100

        valid_tokens = mask.sum().float()
        if valid_tokens.item() > 0:
            correct = (pred_shift == label_shift) & mask
            ma = correct.sum().float() / valid_tokens * 100.0
        else:
            ma = torch.tensor(0.0, device=loss.device)

        name = self.test_dataset_names[dataloader_idx] if dataloader_idx < len(self.test_dataset_names) else f"test/{dataloader_idx}_"

        self.log(f"{name}loss", loss, add_dataloader_idx=False)
        self.log(f"{name}ppl", ppl, add_dataloader_idx=False)
        self.log(f"{name}ma", ma, add_dataloader_idx=False)

        # Optional debug for first test batch
        if batch_idx == 0 and (hasattr(self, "tokenizer") and self.tokenizer is not None):
            try:
                sample_pred = pred_shift[0].cpu().numpy().tolist()
                sample_lab  = label_shift[0].cpu().numpy().tolist()
                sample_lab_tokens = [t for t in sample_lab if t != -100][:20]
                sample_pred_tokens = sample_pred[:len(sample_lab_tokens)]
                print("DEBUG TEST SAMPLE:")
                print("  labels  >", self.tokenizer.decode(sample_lab_tokens, clean_up_tokenization_spaces=True))
                print("  predict >", self.tokenizer.decode([int(t) for t in sample_pred_tokens], clean_up_tokenization_spaces=True))
            except Exception:
                pass

        return {"loss": loss, "ppl": ppl, "ma": ma}


    def on_validation_epoch_end(self):
        metrics = self.trainer.logged_metrics

        # Aggregate forget vs retain metrics
        retain_ppl_vals = [v for k, v in metrics.items() if "ppl" in k and "forget" not in k and "val" in k and isinstance(v, torch.Tensor)]
        forget_ppl_vals = [v for k, v in metrics.items() if "forget" in k and "ppl" in k and "val" in k and isinstance(v, torch.Tensor)]

        # Aggregate MA metrics
        retain_ma_vals = [v for k, v in metrics.items() if "ma" in k and "forget" not in k and "val" in k and isinstance(v, torch.Tensor)]
        forget_ma_vals = [v for k, v in metrics.items() if "forget" in k and "ma" in k and "val" in k and isinstance(v, torch.Tensor)]

        if retain_ppl_vals:
            avg_retain_ppl = torch.stack(retain_ppl_vals).mean()
            self.log("val/retain_xppl", avg_retain_ppl, prog_bar=True)

            # Track if we're improving on retain set
            if avg_retain_ppl < self.best_retain_ppl:
                self.best_retain_ppl = avg_retain_ppl
                self.log("val/best_retain_ppl", self.best_retain_ppl)

        if forget_ppl_vals:
            avg_forget_ppl = torch.stack(forget_ppl_vals).mean()
            self.log("val/forget_xppl", avg_forget_ppl, prog_bar=True)

        # Log average MA
        if retain_ma_vals:
            avg_retain_ma = torch.stack(retain_ma_vals).mean()
            self.log("val/retain_xma", avg_retain_ma, prog_bar=True)

        if forget_ma_vals:
            avg_forget_ma = torch.stack(forget_ma_vals).mean()
            self.log("val/forget_xma", avg_forget_ma, prog_bar=True)

        # CRITICAL: Log the ratio (we want this to increase)
        if retain_ppl_vals and forget_ppl_vals:
            ratio = torch.stack(forget_ppl_vals).mean() / torch.stack(retain_ppl_vals).mean()
            self.log("val/forget_to_retain_ratio", ratio, prog_bar=True)

        # MA ratio (we want forget MA to be LOWER than retain MA)
        if retain_ma_vals and forget_ma_vals:
            ma_gap = torch.stack(retain_ma_vals).mean() - torch.stack(forget_ma_vals).mean()
            self.log("val/ma_gap", ma_gap, prog_bar=True)  # Positive = good forgetting

    def configure_optimizers(self):
        # Use lower learning rate for stability
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )

        total_steps = self.trainer.estimated_stepping_batches
        warmup_steps = int(total_steps * self.args.warmup_ratio)

        scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)

        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "interval": "step"}}

print("‚úì Improved Model created")

from argparse import Namespace

args = Namespace(
    # Model
    model_name_or_path="gpt2", # Changed from bigscience/bloom-560m to a smaller model
    model_type="gpt2",
    cache_dir="/content/drive/MyDrive/indic_unlearning/cache",

    # Data
    data_dir="/content/drive/MyDrive/indic_unlearning/data",
    forget_num=50,
    retain_multiplier=5,
    max_seq_len=128,

    # Languages
    forget_lang=["hi", "bn", "te", "mai", "bho"],
    retain_lang=["hi", "bn","te", "mai", "bho"],
    test_src_lang_only=False,

    # Training
    per_device_train_batch_size=1,  # Further reduced due to OOM
    per_device_eval_batch_size=1,   # Further reduced due to OOM
    gradient_accumulation_steps=8,  # Increased to maintain effective batch size of 8
    learning_rate=1e-5,  # LOWER: More stable
    weight_decay=0.01,
    warmup_ratio=0.1,
    epochs=15,  # More epochs with gentler approach

    # FIXED: Unlearning parameters
    forget_multiplier=1,  # 1:1 ratio (F-R-F-R-F-R)
    alternate_loader_every_n_epoch=1,
    forget_loss_weight=0.05,  # Start target (will warmup from 0.01)

    # NEW: Regularization to prevent collapse
    use_weight_regularization=True,
    weight_reg_strength=0.001,  # Penalty for drifting from original weights

    # NEW: Gradient Checkpointing for memory saving
    use_gradient_checkpointing=True,

    # System
    seed=42,
    output_dir="/content/drive/MyDrive/indic_unlearning/checkpoints_fixed",
)

print("="*60)
print("FIXED CONFIGURATION")
print("="*60)
print(f"Model: {args.model_name_or_path}")
print(f"Languages: {args.forget_lang}")
print(f"Forget: {args.forget_num} samples")
print(f"Retain: {args.forget_num * args.retain_multiplier} samples")
print(f"Epochs: {args.epochs}")
print(f"Per-device Train Batch Size: {args.per_device_train_batch_size}")
print(f"Gradient Accumulation Steps: {args.gradient_accumulation_steps}")
print(f"Effective Training Batch Size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}")
print(f"Per-device Eval Batch Size: {args.per_device_eval_batch_size}")
print(f"Gradient Checkpointing: {args.use_gradient_checkpointing}")

print("="*60)

from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping
from lightning.pytorch.loggers import CSVLogger
import os

L.seed_everything(args.seed)

# Local directories
LOCAL_CKPT_DIR = "/content/checkpoints_fixed"
LOCAL_LOG_DIR = "/content/logs_fixed"
os.makedirs(LOCAL_CKPT_DIR, exist_ok=True)
os.makedirs(LOCAL_LOG_DIR, exist_ok=True)
os.makedirs(args.output_dir, exist_ok=True)
os.makedirs(args.cache_dir, exist_ok=True)

print("Initializing DataModule...")
dm = FLORESDataModule(args)

print("Initializing FIXED Model...")
model = ImprovedUnlearningModel(args)

# FIXED Callbacks: Monitor RETAIN performance to prevent collapse
callbacks = [
    ModelCheckpoint(
        dirpath=LOCAL_CKPT_DIR,
        filename="epoch={epoch:02d}-retain_ppl={val/retain_xppl:.2f}-forget_ppl={val/forget_xppl:.2f}",
        monitor="val/forget_to_retain_ratio",  # We want HIGH ratio
        mode="max",
        save_top_k=3,
        save_last=True,
        auto_insert_metric_name=False,
    ),
    EarlyStopping(
        monitor="val/retain_xppl",  # CRITICAL: Stop if retain performance degrades
        mode="min",
        patience=5,
        min_delta=0.5,
    ),
]

logger = CSVLogger(LOCAL_LOG_DIR, name="logs")

trainer = L.Trainer(
    default_root_dir=args.output_dir,
    accelerator="gpu",
    devices=1,
    precision="16-mixed",
    max_epochs=args.epochs,
    accumulate_grad_batches=args.gradient_accumulation_steps,
    gradient_clip_val=0.5,  # Lower clip for stability
    log_every_n_steps=5,
    val_check_interval=1.0,
    callbacks=callbacks,
    logger=logger,
    reload_dataloaders_every_n_epochs=args.alternate_loader_every_n_epoch,
)

print("\n" + "="*60)
print("STARTING FIXED TRAINING")
print("="*60)
print("Epoch pattern: F-R-F-R-F-R... (1:1 ratio)")
print("Adaptive loss weight: 0.01 \u2192 0.05 over first 30% of training")
print("Early stopping if retain PPL increases")
print("="*60 + "\n")

trainer.fit(model, dm)

print("\n" + "="*60)
print("RUNNING TEST")
print("="*60)
trainer.test(model, dm)

print("\n" + "="*60)
print("TRAINING COMPLETE!")
print("="*60)

# Copy checkpoints and logs to Google Drive
print("\nCopying results to Google Drive...")
import shutil

# Copy checkpoints
for ckpt_file in os.listdir(LOCAL_CKPT_DIR):
    if ckpt_file.endswith('.ckpt'):
        src = os.path.join(LOCAL_CKPT_DIR, ckpt_file)
        dst = os.path.join(args.output_dir, ckpt_file)
        shutil.copy2(src, dst)
        print(f"  ‚úì Copied {ckpt_file}")

# Copy logs
log_src = os.path.join(LOCAL_LOG_DIR, "logs")
log_dst = os.path.join(args.output_dir, "logs")
if os.path.exists(log_src):
    shutil.copytree(log_src, log_dst, dirs_exist_ok=True)
    print(f"  ‚úì Copied logs")

print(f"\nCheckpoints saved to: {args.output_dir}")

import pandas as pd
import matplotlib.pyplot as plt

metrics_path = f"{LOCAL_LOG_DIR}/logs/version_0/metrics.csv"

if os.path.exists(metrics_path):
    df = pd.read_csv(metrics_path)

    fig, axes = plt.subplots(3, 3, figsize=(18, 14))

    # Plot 1: Training loss with adaptive weight
    ax1 = axes[0, 0]
    if 'train/loss' in df.columns:
        df['train/loss'].dropna().plot(ax=ax1, label='Total Loss')
    if 'train/adaptive_weight' in df.columns:
        ax1_twin = ax1.twinx()
        df['train/adaptive_weight'].dropna().plot(ax=ax1_twin, label='Adaptive Weight', color='orange', linestyle='--')
        ax1_twin.set_ylabel('Adaptive Weight', color='orange')
    ax1.set_title('Training Loss + Adaptive Weight')
    ax1.legend(loc='upper left')

    # Plot 2: Forget vs Retain loss
    ax2 = axes[0, 1]
    for col in ['train/forget_loss', 'train/retain_loss']:
        if col in df.columns:
            df[col].dropna().plot(ax=ax2, label=col.replace('train/', ''))
    ax2.set_title('Forget vs Retain Loss')
    ax2.legend()

    # Plot 3: Validation PPL comparison
    ax3 = axes[0, 2]
    if 'val/retain_xppl' in df.columns:
        df['val/retain_xppl'].dropna().plot(ax=ax3, label='Retain PPL', color='green', linewidth=2)
    if 'val/forget_xppl' in df.columns:
        df['val/forget_xppl'].dropna().plot(ax=ax3, label='Forget PPL', color='red', linewidth=2)
    ax3.set_title('Validation: Forget vs Retain PPL')
    ax3.legend()
    ax3.set_yscale('log')

    # Plot 4: Forget-to-Retain Ratio (KEY METRIC)
    ax4 = axes[1, 0]
    if 'val/forget_to_retain_ratio' in df.columns:
        df['val/forget_to_retain_ratio'].dropna().plot(ax=ax4, color='purple', linewidth=2)
        ax4.axhline(y=1.0, color='gray', linestyle='--', label='Ratio=1 (no forgetting)')
        ax4.set_title('Forget/Retain PPL Ratio (Higher = Better)')
        ax4.legend()

    # Plot 5: Memorization Accuracy (MA) - CRITICAL METRIC
    ax5 = axes[1, 1]
    if 'val/retain_xma' in df.columns:
        df['val/retain_xma'].dropna().plot(ax=ax5, label='Retain MA', color='green', linewidth=2)
    if 'val/forget_xma' in df.columns:
        df['val/forget_xma'].dropna().plot(ax=ax5, label='Forget MA', color='red', linewidth=2)
    ax5.set_title('Memorization Accuracy (MA)')
    ax5.set_ylabel('MA (%)')
    ax5.legend()
    ax5.axhline(y=50, color='gray', linestyle='--', alpha=0.5)

    # Plot 6: MA Gap (Retain - Forget)
    ax6 = axes[1, 2]
    if 'val/ma_gap' in df.columns:
        df['val/ma_gap'].dropna().plot(ax=ax6, color='blue', linewidth=2)
        ax6.axhline(y=0, color='gray', linestyle='--', label='No gap')
        ax6.set_title('MA Gap (Retain - Forget) [Higher = Better]')
        ax6.legend()

    # Plot 7: Weight drift (if regularization used)
    ax7 = axes[2, 0]
    if 'train/weight_drift' in df.columns:
        df['train/weight_drift'].dropna().plot(ax=ax7, color='brown')
        ax7.set_title('Weight Drift from Original')

    # Plot 8: Final test PPL
    ax8 = axes[2, 1]
    test_cols = [c for c in df.columns if 'test' in c and 'ppl' in c]
    if test_cols:
        final_test = df[test_cols].iloc[-1].dropna()
        colors = ['red' if 'forget' in c else 'green' for c in final_test.index]
        final_test.plot(kind='bar', ax=ax8, color=colors)
        ax8.set_title('Final Test PPL')
        plt.setp(ax8.xaxis.get_majorticklabels(), rotation=45, ha='right')

    # Plot 9: Final test MA
    ax9 = axes[2, 2]
    test_ma_cols = [c for c in df.columns if 'test' in c and 'ma' in c]
    if test_ma_cols:
        final_test_ma = df[test_ma_cols].iloc[-1].dropna()
        colors = ['red' if 'forget' in c else 'green' for c in final_test_ma.index]
        final_test_ma.plot(kind='bar', ax=ax9, color=colors)
        ax9.set_title('Final Test MA (%)')
        ax9.axhline(y=50, color='gray', linestyle='--', alpha=0.5)
        plt.setp(ax9.xaxis.get_majorticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.savefig(f"{args.output_dir}/results_fixed.png", dpi=150, bbox_inches='tight')
    plt.show()

    # Summary
    print("\n" + "="*60)
    print("FIXED RESULTS SUMMARY")
    print("="*60)

    final_metrics = df.iloc[-1].dropna()

    print("\nüìä Final Test Metrics:")
    for col, val in sorted(final_metrics.items()):
        if 'test' in col and ('ppl' in col or 'loss' in col or 'ma' in col):
            print(f"  {col}: {val:.4f}")

    print("\n" + "="*60)
    print("INTERPRETATION")
    print("="*60)

    # Calculate success metrics
    forget_ppls = [v for k, v in final_metrics.items() if 'test' in k and 'forget' in k and 'ppl' in k]
    normal_ppls = [v for k, v in final_metrics.items() if 'test' in k and 'ppl' in k and 'forget' not in k]
    forget_mas = [v for k, v in final_metrics.items() if 'test' in k and 'forget' in k and 'ma' in k]
    normal_mas = [v for k, v in final_metrics.items() if 'test' in k and 'ma' in k and 'forget' not in k]

    if forget_ppls and normal_ppls:
        avg_forget = sum(forget_ppls) / len(forget_ppls)
        avg_normal = sum(normal_ppls) / len(normal_ppls)
        ratio = avg_forget / avg_normal

        print(f"\nüìà Perplexity Metrics:")
        print(f"  ‚úÖ Average Forget PPL: {avg_forget:.2f}")
        print(f"  ‚úÖ Average Normal PPL: {avg_normal:.2f}")
        print(f"  ‚úÖ Forget/Normal Ratio: {ratio:.2f}x")

    if forget_mas and normal_mas:
        avg_forget_ma = sum(forget_mas) / len(forget_mas)
        avg_normal_ma = sum(normal_mas) / len(normal_mas)
        ma_gap = avg_normal_ma - avg_forget_ma

        print(f"\nüéØ Memorization Accuracy (MA) Metrics:")
        print(f"  ‚úÖ Average Retain MA: {avg_normal_ma:.2f}%")
        print(f"  ‚úÖ Average Forget MA: {avg_forget_ma:.2f}%")
        print(f"  ‚úÖ MA Gap (Retain - Forget): {ma_gap:.2f}%")

        if ma_gap > 10:
            print(f"\nüéâ EXCELLENT! Model shows strong selective forgetting")
            print(f"   Forget set MA is {ma_gap:.1f}% lower than retain set")
        elif ma_gap > 5:
            print(f"\n‚úÖ GOOD: Moderate selective forgetting")
            print(f"   MA gap of {ma_gap:.1f}% indicates some forgetting")
        elif ma_gap > 0:
            print(f"\n‚ö†Ô∏è  WEAK: Minimal forgetting detected")
            print(f"   MA gap of {ma_gap:.1f}% is quite small")
        # else:
        #     print(f"\n‚ùå FAILURE: No forgetting (or model degraded)")
        #     print(f"   Negative MA gap indicates issues")

    # Overall assessment
    print("\n" + "="*60)
    print("OVERALL ASSESSMENT")
    print("="*60)

    # if forget_ppls and normal_ppls and forget_mas and normal_mas:
    #     ppl_success = ratio > 1.5
    #     ma_success = ma_gap > 5
    #     utility_ok = avg_normal_ma > 40  # Model still functional

        # if ppl_success and ma_success and utility_ok:
        #     print("üéâ SUCCESS! Both PPL and MA show selective forgetting")
        #     print("   Model maintains utility while forgetting target data")
        # elif ppl_success or ma_success:
        #     print("‚ö†Ô∏è  PARTIAL SUCCESS: One metric shows forgetting")
        #     print("   Consider tuning hyperparameters for better results")
        # else:
        #     print("‚ùå FAILURE: Neither metric shows strong forgetting")
        #     print("   Increase forget_loss_weight or train longer")

    #     print(f"\nüìä Compared to ORIGINAL (all 1M PPL):")
    #     print(f"  - PPL values: {avg_normal:.0f} vs 1,000,000 = {1000000/avg_normal:.0f}x better!")
    #     print(f"  - Model functional: {'YES ‚úì' if utility_ok else 'NO ‚úó'}")
    #     print(f"  - Selective forgetting: {'YES ‚úì' if (ppl_success or ma_success) else 'NO ‚úó'}")

else:
    print(f"‚úó Metrics not found: {metrics_path}")

results = {
    "config": {
        "model": args.model_name_or_path,
        "languages": args.forget_lang,
        "fixes_applied": [
            "1:1 forget/retain ratio (was 3:1)",
            "Adaptive loss weight warmup (0.01 \u2192 0.05)",
            "Weight regularization (strength=0.001)",
            "Lower learning rate (1e-5 vs 2e-5)",
            "Better loss capping (20 vs 100)",
            "Early stopping on retain performance"
        ],
        "forget_num": args.forget_num,
        "retain_multiplier": args.retain_multiplier,
        "epochs": args.epochs,
    },
    "final_metrics": {k: float(v) for k, v in final_metrics.items() if 'ppl' in k or 'loss' in k} if 'final_metrics' in dir() else {},
}

with open(f"{args.output_dir}/results_fixed_summary.json", "w") as f:
    json.dump(results, f, indent=2, default=str)

print(f"\n‚úì Results saved to: {args.output_dir}/results_fixed_summary.json")
print(f"‚úì Plot saved to: {args.output_dir}/results_fixed.png")

