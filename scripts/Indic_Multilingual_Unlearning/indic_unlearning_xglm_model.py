# -*- coding: utf-8 -*-
"""Indic Unlearning XGLM Final Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VsUKK6rlNLEPWxEpYuIWDf42iu7KTtRx

# Cross-Lingual Unlearning
"""

!pip install -q transformers==4.36.0 lightning==2.1.3 torchmetrics pandas matplotlib
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.set_float32_matmul_precision('high')
torch.cuda.empty_cache()
!nvidia-smi --query-gpu=name,memory.total --format=csv
print("âœ… Setup complete")

import json, random, copy, gc
from typing import List
from dataclasses import dataclass
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import lightning as L
from lightning.pytorch.callbacks import ModelCheckpoint, Callback
from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup
print("âœ… Imports")

@dataclass
class Config:
    model_name: str = "facebook/xglm-564M"
    languages: List[str] = None
    data_dir: str = "/content/data"
    output_dir: str = "/content/ckpts"
    cache_dir: str = "/content/cache"

    forget_num: int = 200
    retain_multiplier: int = 5
    max_seq_len: int = 256

    # CORRECTED: Less aggressive unlearning
    learning_rate: float = 1e-4  # Reduced from 5e-4
    warmup_ratio: float = 0.1
    temperature: float = 1.0
    lambda_forget: float = 0.3  # Scale down forget loss (gradient ascent)

    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 2
    gradient_accumulation_steps: int = 8

    max_epochs: int = 10  # More epochs for gradual forgetting
    forget_every_n_epochs: int = 2  # Forget every 4th epoch (0,4,8,12...)

    num_workers: int = 2
    seed: int = 42
    ma_max_samples: int = 10
    ma_stride: int = 16

    def __post_init__(self):
        if self.languages is None:
            self.languages = ["hi","bn","te","ta","mr","gu","kn","ml","pa","ur","or","as","sd","bho","mai","san","kas_ar","kas_de"]

config = Config()
for d in [config.data_dir, config.output_dir, config.cache_dir]:
    os.makedirs(d, exist_ok=True)
L.seed_everything(config.seed)
print(f"Model: {config.model_name}")
print(f"LR: {config.learning_rate} (less aggressive)")
print(f"Forget schedule: Every {config.forget_every_n_epochs} epochs")
print(f"Lambda forget: {config.lambda_forget} (scaled down)")
print(f"Max epochs: {config.max_epochs}")
print("âœ… Config")

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
print("Upload: forget-200.jsonl, retain-200-x5.jsonl, test.jsonl, valid.jsonl")
uploaded = files.upload()
for f in uploaded:
    os.rename(f, os.path.join(config.data_dir, f))
print("âœ… Uploaded")

class FLORESDataset(Dataset):
    def __init__(self, data, tokenizer, max_len=256, languages=["hi"]):
        self.data, self.tokenizer, self.max_len, self.languages = data, tokenizer, max_len, languages
    def __len__(self): return len(self.data)
    def __getitem__(self, idx):
        lang = random.choice(self.languages) if len(self.languages)>1 else self.languages[0]
        enc = self.tokenizer(self.data[idx][lang], max_length=self.max_len, padding="max_length", truncation=True, return_tensors="pt")
        labels = enc["input_ids"].clone()
        labels[labels==self.tokenizer.pad_token_id] = -100
        return {"input_ids":enc["input_ids"].squeeze(0), "attention_mask":enc["attention_mask"].squeeze(0), "labels":labels.squeeze(0)}

class FLORESDataModule(L.LightningDataModule):
    def __init__(self, cfg, tok):
        super().__init__()
        self.cfg, self.tok = cfg, tok
    def load(self, f): return [json.loads(l) for l in open(f"{self.cfg.data_dir}/{f}")]
    def setup(self, stage=None):
        if stage=="fit" or stage is None:
            fd = self.load(f"forget-{self.cfg.forget_num}.jsonl")
            rd = self.load(f"retain-{self.cfg.forget_num}-x{self.cfg.retain_multiplier}.jsonl")
            vd = self.load("valid.jsonl")
            self.forget = FLORESDataset(fd, self.tok, self.cfg.max_seq_len, self.cfg.languages)
            self.retain = FLORESDataset(rd, self.tok, self.cfg.max_seq_len, self.cfg.languages)
            self.valid = []
            for lang in self.cfg.languages:
                self.valid.append(FLORESDataset(vd, self.tok, self.cfg.max_seq_len, [lang]))
                self.valid.append(FLORESDataset(fd, self.tok, self.cfg.max_seq_len, [lang]))
        if stage=="test" or stage is None:
            fd = self.load(f"forget-{self.cfg.forget_num}.jsonl")
            td = self.load("test.jsonl")
            self.test = []
            for lang in self.cfg.languages:
                self.test.append(FLORESDataset(td, self.tok, self.cfg.max_seq_len, [lang]))
                self.test.append(FLORESDataset(fd, self.tok, self.cfg.max_seq_len, [lang]))
    def train_dataloader(self):
        # CORRECTED: Forget every N epochs, otherwise retain
        is_forget_epoch = self.trainer.current_epoch % self.cfg.forget_every_n_epochs == 0
        ds = self.forget if is_forget_epoch else self.retain
        return DataLoader(ds, batch_size=self.cfg.per_device_train_batch_size, num_workers=self.cfg.num_workers, shuffle=True, pin_memory=True)
    def val_dataloader(self):
        return [DataLoader(d, batch_size=self.cfg.per_device_eval_batch_size, num_workers=self.cfg.num_workers, shuffle=False, pin_memory=True) for d in self.valid]
    def test_dataloader(self):
        return [DataLoader(d, batch_size=self.cfg.per_device_eval_batch_size, num_workers=self.cfg.num_workers, shuffle=False, pin_memory=True) for d in self.test]

print("âœ… Dataset")

class Model(L.LightningModule):
    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.save_hyperparameters()
        self.tok = AutoTokenizer.from_pretrained(cfg.model_name, cache_dir=cfg.cache_dir)
        if self.tok.pad_token is None: self.tok.pad_token = self.tok.eos_token
        self.model = AutoModelForCausalLM.from_pretrained(cfg.model_name, cache_dir=cfg.cache_dir)
        self.model.gradient_checkpointing_enable()
        self.teacher = copy.deepcopy(self.model)
        self.teacher.eval()
        for p in self.teacher.parameters(): p.requires_grad=False
        self.vnames = [f"val/{l}_{t}_" for l in cfg.languages for t in ["valid","forget"]]
        self.tnames = [f"test/{l}_{t}_" for l in cfg.languages for t in ["test","forget"]]
        self.val_ma_correct, self.val_ma_total = [], []
        self.test_ma_correct, self.test_ma_total = [], []

    def forward(self, **x): return self.model(**x)

    def training_step(self, batch, batch_idx):
        out = self(**batch)
        loss = out.loss
        # CORRECTED: Check if this is a forget epoch
        is_forget = self.current_epoch % self.cfg.forget_every_n_epochs == 0

        if is_forget:
            # Gradient ascent with scaling
            loss = -loss * self.cfg.lambda_forget  # Scale down
            self.log("train/forget_loss", loss, prog_bar=True)
        else:
            # Retention with KD
            logit_s = out.logits
            mask = batch["labels"].eq(-100)
            with torch.no_grad():
                logit_t = self.teacher(**batch).logits
            prob_t = F.softmax(logit_t, dim=-1)
            lbl = torch.clamp(batch["labels"], min=0)
            pt = prob_t.gather(-1, lbl.unsqueeze(-1))
            pt.masked_fill_(mask.unsqueeze(-1), 0)
            kappa = (pt.sum()/(~mask).sum()).clamp(0,1)
            lkd = F.kl_div(F.log_softmax(logit_s/self.cfg.temperature,-1), F.softmax(logit_t/self.cfg.temperature,-1), reduction="batchmean")*(self.cfg.temperature**2)
            del logit_t, prob_t
            loss = kappa*lkd + (1-kappa)*loss
            self.log_dict({"train/loss":loss,"train/kd":lkd,"train/kappa":kappa}, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx, dataloader_idx=0):
        loss = self(**batch).loss
        ppl = torch.exp(loss.clamp(max=10))
        name = self.vnames[dataloader_idx] if dataloader_idx<len(self.vnames) else f"val/unk{dataloader_idx}_"
        corr, tot = self._ma_counts(batch)
        while len(self.val_ma_correct) <= dataloader_idx:
            self.val_ma_correct.append(0)
            self.val_ma_total.append(0)
        self.val_ma_correct[dataloader_idx] += corr
        self.val_ma_total[dataloader_idx] += tot
        self.log_dict({f"{name}ppl":ppl, f"{name}loss":loss}, add_dataloader_idx=False)
        torch.cuda.empty_cache()
        return loss

    def test_step(self, batch, batch_idx, dataloader_idx=0):
        loss = self(**batch).loss
        ppl = torch.exp(loss.clamp(max=10))
        name = self.tnames[dataloader_idx] if dataloader_idx<len(self.tnames) else f"test/unk{dataloader_idx}_"
        corr, tot = self._ma_counts(batch)
        while len(self.test_ma_correct) <= dataloader_idx:
            self.test_ma_correct.append(0)
            self.test_ma_total.append(0)
        self.test_ma_correct[dataloader_idx] += corr
        self.test_ma_total[dataloader_idx] += tot
        self.log_dict({f"{name}ppl":ppl, f"{name}loss":loss}, add_dataloader_idx=False)
        torch.cuda.empty_cache()
        return loss

    @torch.no_grad()
    def _ma_counts(self, batch):
        try:
            bs = batch["input_ids"].size(0)
            if bs > self.cfg.ma_max_samples:
                idx = torch.randperm(bs, device=batch["input_ids"].device)[:self.cfg.ma_max_samples]
                batch = {k:v[idx] if torch.is_tensor(v) else v for k,v in batch.items()}
            corr, tot = 0, 0
            for pos in range(self.cfg.ma_stride, self.cfg.max_seq_len, self.cfg.ma_stride):
                lbl = batch["labels"][...,pos]
                if (lbl==-100).all(): break
                out = self.model(input_ids=batch["input_ids"][...,:pos], attention_mask=batch["attention_mask"][...,:pos])
                pred = out.logits[:,-1,:].argmax(-1)
                valid = lbl!=-100
                corr += ((pred==lbl)&valid).sum().item()
                tot += valid.sum().item()
                del out
                if pos%(self.cfg.ma_stride*3)==0: torch.cuda.empty_cache()
            torch.cuda.empty_cache()
            return corr, tot
        except: return 0, 1

    def on_validation_epoch_end(self):
        for i, (c, t) in enumerate(zip(self.val_ma_correct, self.val_ma_total)):
            if t>0:
                self.log(f"{self.vnames[i] if i<len(self.vnames) else f'val/unk{i}_'}ma", c/t)
        fidx = [i for i,n in enumerate(self.vnames) if "forget" in n]
        if fidx:
            tc = sum(self.val_ma_correct[i] for i in fidx)
            tt = sum(self.val_ma_total[i] for i in fidx)
            if tt>0:
                fma = tc/tt
                self.log("val/forget_xma", fma)
                print(f"\nðŸ“Š Epoch {self.current_epoch} Forget MA: {fma:.2%}")
        self.val_ma_correct, self.val_ma_total = [], []
        m = self.trainer.logged_metrics
        fppl = [v for k,v in m.items() if "ppl" in k and "forget" in k]
        if fppl: self.log("val/forget_xppl", torch.stack(fppl).mean())
        torch.cuda.empty_cache(); gc.collect()

    def on_test_epoch_end(self):
        for i, (c,t) in enumerate(zip(self.test_ma_correct, self.test_ma_total)):
            if t>0:
                self.log(f"{self.tnames[i] if i<len(self.tnames) else f'test/unk{i}_'}ma", c/t)
        self.test_ma_correct, self.test_ma_total = [], []

    def configure_optimizers(self):
        opt = torch.optim.AdamW(self.model.parameters(), lr=self.cfg.learning_rate)
        sch = get_linear_schedule_with_warmup(opt, int(self.cfg.warmup_ratio*self.trainer.estimated_stepping_batches), self.trainer.estimated_stepping_batches)
        return {"optimizer":opt, "lr_scheduler":{"scheduler":sch,"interval":"step"}}

print("âœ… Model")

class CB(Callback):
    def __init__(self, out): self.out = out
    def on_test_epoch_end(self, trainer, _):
        pd.DataFrame({k.replace("test/",""):[v.item()] for k,v in trainer.logged_metrics.items()}).to_csv(f"{self.out}/test.csv", index=False)

tok = AutoTokenizer.from_pretrained(config.model_name, cache_dir=config.cache_dir)
if tok.pad_token is None: tok.pad_token = tok.eos_token
dm = FLORESDataModule(config, tok)
dm.setup("fit")
print(f"Forget:{len(dm.forget)} Retain:{len(dm.retain)} Valid:{len(dm.valid)}")
model = Model(config)
print(f"Params: {sum(p.numel() for p in model.model.parameters())/1e6:.0f}M")
ckpt = ModelCheckpoint(config.output_dir, "e{epoch:02d}-fma{val/forget_xma:.3f}", "val/forget_xma", "min", save_top_k=2, save_weights_only=True)
trainer = L.Trainer(default_root_dir=config.output_dir, accelerator="gpu", devices=1, precision="bf16-mixed", max_epochs=config.max_epochs,
                    accumulate_grad_batches=config.gradient_accumulation_steps, gradient_clip_val=1.0,
                    callbacks=[ckpt, CB(config.output_dir)], reload_dataloaders_every_n_epochs=1, num_sanity_val_steps=0)
print("âœ… Ready")

print("="*80)
print("ðŸš€ TRAINING (Corrected for 25-30% MA)")
print("="*80)
print(f"Forget every {config.forget_every_n_epochs} epochs (0, 4, 8, 12...)")
print(f"Retain on other epochs (1,2,3, 5,6,7, 9,10,11...)")
print(f"Lambda forget: {config.lambda_forget} (scaled down gradient ascent)")
print("="*80 + "\n")
trainer.fit(model, dm)
print("\n"+"="*80)
print("âœ… DONE")
print("="*80)

import torch

# Allow Config class to be unpickled
torch.serialization.add_safe_globals([Config])

# Now load checkpoint
print("ðŸ“Š TESTING")
best = Model.load_from_checkpoint(ckpt.best_model_path, cfg=config)
dm.setup("test")
trainer.test(best, dm)
print("âœ… Done")

print("="*80)
print("ðŸ“Š COMPUTING MA FROM TEST RESULTS")
print("="*80)

# The test already ran, we just need to extract MA from the logged metrics
# Let's re-run test but this time ensure MA is computed

# Reload best model
torch.serialization.add_safe_globals([Config])
best = Model.load_from_checkpoint(ckpt.best_model_path, cfg=config)

# Setup test data
dm.setup("test")

# Create new callback that saves AFTER MA computation
class MACallback(Callback):
    def on_test_end(self, trainer, pl_module):
        metrics = {}
        for k, v in trainer.logged_metrics.items():
            if k.startswith('test/'):
                clean_key = k.replace('test/', '')
                metrics[clean_key] = v.item() if hasattr(v, 'item') else v

        if metrics:
            df = pd.DataFrame([metrics])
            df.to_csv(f"{config.output_dir}/test_with_ma.csv", index=False)
            print(f"\nðŸ’¾ Saved {len(metrics)} metrics including MA")

            # Show MA columns
            ma_cols = [c for c in df.columns if '_ma' in c]
            print(f"MA columns: {ma_cols}")

# Create new trainer with MA callback
test_trainer = L.Trainer(
    default_root_dir=config.output_dir,
    accelerator="gpu",
    devices=1,
    precision="bf16-mixed",
    callbacks=[MACallback()],
    enable_progress_bar=True
)

print("\nðŸ”„ Re-running test with MA computation...")
test_trainer.test(best, dm)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from pathlib import Path

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

# ============================================================================
# EXTRACT AND ORGANIZE RESULTS
# ============================================================================

print("="*80)
print("ðŸ“Š INDIC LANGUAGE UNLEARNING - COMPREHENSIVE ANALYSIS")
print("="*80)

# Get metrics from trainer
all_metrics = {}
if hasattr(trainer, 'callback_metrics'):
    for key, value in trainer.callback_metrics.items():
        if isinstance(value, torch.Tensor):
            all_metrics[key] = value.item()
        else:
            all_metrics[key] = float(value)

# Clean and organize results
results_dict = {}
for key, value in all_metrics.items():
    # Remove 'test/' prefix
    clean_key = key.replace('test/', '')
    if '_test_ma' in clean_key or '_forget_ma' in clean_key:
        results_dict[clean_key] = value

# Extract unique languages
languages = sorted(list(set([k.split('_')[0] for k in results_dict.keys()
                             if '_test_ma' in k or '_forget_ma' in k])))

print(f"\nðŸ“‹ Found {len(languages)} languages: {', '.join(languages)}")

# Create detailed results dataframe
detailed_results = []
for lang in languages:
    test_key = f"{lang}_test_ma"
    forget_key = f"{lang}_forget_ma"

    if test_key in results_dict and forget_key in results_dict:
        test_val = results_dict[test_key] * 100
        forget_val = results_dict[forget_key] * 100

        detailed_results.append({
            "Language": lang.upper(),
            "Language_Code": lang,
            "Test MA (%)": round(test_val, 2),
            "Forget MA (%)": round(forget_val, 2),
            "Utility Retained (%)": round(test_val, 2),
            "Forgetting Effectiveness (%)": round(100 - forget_val, 2),
            "Utility-Forgetting Ratio": round(test_val / forget_val if forget_val > 0 else 0, 2)
        })

results_df = pd.DataFrame(detailed_results)
results_df = results_df.sort_values('Forgetting Effectiveness (%)', ascending=False)

# ============================================================================
# DISPLAY RESULTS TABLE
# ============================================================================

print("\n" + "="*80)
print("ðŸ“‹ DETAILED RESULTS BY LANGUAGE")
print("="*80)
print(results_df.to_string(index=False))